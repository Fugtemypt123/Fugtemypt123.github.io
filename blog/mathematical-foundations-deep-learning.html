<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Mathematical Foundations of Deep Learning - Shaofeng Yin</title>
    <meta name="author" content="Shaofeng Yin">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="../images/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="../stylesheet.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  </head>

  <body>
    <!-- Navigation Bar -->
    <div class="nav-container">
      <nav class="navbar">
        <div class="nav-content">
          <div class="nav-logo">
            <a href="../index.html">Shaofeng Yin</a>
          </div>
          <ul class="nav-menu">
            <li class="nav-item">
              <a href="../index.html" class="nav-link">Home</a>
            </li>
            <li class="nav-item">
              <a href="../blog.html" class="nav-link active">Blog</a>
            </li>
          </ul>
        </div>
      </nav>
    </div>

    <div class="blog-post">
      <a href="../blog.html" class="back-to-blog">← Back to Blog</a>
      
      <div class="blog-post-header">
        <h1 class="blog-post-title">Mathematical Foundations of Deep Learning</h1>
        <div class="blog-post-meta">December 10, 2024 • 15 min read</div>
      </div>

      <div class="blog-post-content">
        <p>
          Deep learning has revolutionized artificial intelligence, but its success is built upon solid 
          mathematical foundations. Understanding these principles is crucial for developing better models 
          and avoiding common pitfalls. In this post, we'll explore the key mathematical concepts that 
          underpin modern neural networks.
        </p>

        <h2>Linear Algebra: The Foundation</h2>
        
        <p>
          At the heart of neural networks lies linear algebra. Every layer transformation can be viewed 
          as a matrix operation. Consider a simple linear layer:
        </p>

        <p style="text-align: center;">
          \[
          \mathbf{y} = \mathbf{W}\mathbf{x} + \mathbf{b}
          \]
        </p>

        <p>
          Where \(\mathbf{W}\) is the weight matrix, \(\mathbf{x}\) is the input vector, \(\mathbf{b}\) 
          is the bias vector, and \(\mathbf{y}\) is the output. The dimensions must align properly: 
          if \(\mathbf{x} \in \mathbb{R}^{n}\) and we want \(\mathbf{y} \in \mathbb{R}^{m}\), then 
          \(\mathbf{W} \in \mathbb{R}^{m \times n}\) and \(\mathbf{b} \in \mathbb{R}^{m}\).
        </p>

        <h3>Eigenvalues and Eigenvectors</h3>
        
        <p>
          Understanding eigenvalues is crucial for analyzing neural network dynamics. The eigenvalues 
          of the weight matrix determine the stability of the network during training. If all eigenvalues 
          have magnitude less than 1, the network is stable; if any eigenvalue has magnitude greater than 1, 
          gradients can explode.
        </p>

        <p style="text-align: center;">
          \[
          \mathbf{W}\mathbf{v} = \lambda\mathbf{v}
          \]
        </p>

        <h2>Calculus: Gradient Descent and Optimization</h2>
        
        <p>
          Training neural networks relies heavily on calculus, particularly the chain rule for computing 
          gradients. The gradient of the loss function with respect to the parameters guides the optimization process.
        </p>

        <h3>The Chain Rule</h3>
        
        <p>
          For a composite function \(f(g(x))\), the derivative is:
        </p>

        <p style="text-align: center;">
          \[
          \frac{df}{dx} = \frac{df}{dg} \cdot \frac{dg}{dx}
          \]
        </p>

        <p>
          In neural networks, this becomes the backpropagation algorithm. For a network with loss \(L\) 
          and parameters \(\theta\), we compute:
        </p>

        <p style="text-align: center;">
          \[
          \frac{\partial L}{\partial \theta} = \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial \theta}
          \]
        </p>

        <h3>Gradient Descent Update Rule</h3>
        
        <p>
          The parameter update rule in gradient descent is:
        </p>

        <p style="text-align: center;">
          \[
          \theta_{t+1} = \theta_t - \alpha \nabla_\theta L(\theta_t)
          \]
        </p>

        <p>
          Where \(\alpha\) is the learning rate and \(\nabla_\theta L(\theta_t)\) is the gradient 
          of the loss function with respect to the parameters at time step \(t\).
        </p>

        <h2>Probability and Statistics</h2>
        
        <p>
          Neural networks are fundamentally probabilistic models. Understanding probability theory 
          helps us interpret model outputs and design better loss functions.
        </p>

        <h3>Maximum Likelihood Estimation</h3>
        
        <p>
          Many loss functions can be derived from maximum likelihood estimation. For a dataset 
          \(\mathcal{D} = \{(x_i, y_i)\}_{i=1}^N\), we maximize:
        </p>

        <p style="text-align: center;">
          \[
          \mathcal{L}(\theta) = \sum_{i=1}^N \log p(y_i | x_i, \theta)
          \]
        </p>

        <h3>Cross-Entropy Loss</h3>
        
        <p>
          For classification tasks, cross-entropy loss is commonly used:
        </p>

        <p style="text-align: center;">
          \[
          L = -\sum_{i=1}^C y_i \log(\hat{y}_i)
          \]
        </p>

        <p>
          Where \(C\) is the number of classes, \(y_i\) is the true label (one-hot encoded), 
          and \(\hat{y}_i\) is the predicted probability for class \(i\).
        </p>

        <h2>Information Theory</h2>
        
        <p>
          Information theory provides insights into model capacity, regularization, and the 
          fundamental limits of learning.
        </p>

        <h3>Entropy and Mutual Information</h3>
        
        <p>
          The entropy of a random variable \(X\) is:
        </p>

        <p style="text-align: center;">
          \[
          H(X) = -\sum_x p(x) \log p(x)
          \]
        </p>

        <p>
          Mutual information between two random variables \(X\) and \(Y\) measures their dependence:
        </p>

        <p style="text-align: center;">
          \[
          I(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)
          \]
        </p>

        <h2>Optimization Theory</h2>
        
        <p>
          Understanding optimization helps us choose better algorithms and understand convergence properties.
        </p>

        <h3>Convexity and Convergence</h3>
        
        <p>
          A function \(f\) is convex if for all \(x, y\) and \(\lambda \in [0, 1]\):
        </p>

        <p style="text-align: center;">
          \[
          f(\lambda x + (1-\lambda)y) \leq \lambda f(x) + (1-\lambda)f(y)
          \]
        </p>

        <p>
          While neural network loss functions are typically non-convex, understanding convex optimization 
          provides insights into local minima and saddle points.
        </p>

        <h3>Learning Rate Scheduling</h3>
        
        <p>
          Adaptive learning rates can improve convergence. A common schedule is:
        </p>

        <p style="text-align: center;">
          \[
          \alpha_t = \frac{\alpha_0}{\sqrt{t}}
          \]
        </p>

        <p>
          Where \(\alpha_0\) is the initial learning rate and \(t\) is the time step.
        </p>

        <h2>Conclusion</h2>
        
        <p>
          The mathematical foundations of deep learning span multiple disciplines: linear algebra for 
          efficient computation, calculus for optimization, probability for modeling uncertainty, 
          information theory for understanding capacity, and optimization theory for convergence analysis.
        </p>

        <p>
          While modern deep learning frameworks abstract away much of this complexity, understanding 
          the underlying mathematics is essential for:
        </p>

        <ul>
          <li>Designing better architectures</li>
          <li>Debugging training issues</li>
          <li>Interpreting model behavior</li>
          <li>Developing new algorithms</li>
        </ul>

        <p>
          As the field continues to evolve, these mathematical principles will remain fundamental 
          to advancing the state of the art in artificial intelligence.
        </p>

        <blockquote>
          "Mathematics is the language in which God has written the universe." - Galileo Galilei
        </blockquote>
      </div>
    </div>
  </body>
</html> 